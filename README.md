# ARES System User Guide

ARES (AI Research Ensemble System) is an automated algorithm evolution and discovery system powered by Large Language Models (LLMs). Through multi-role collaboration (Theorist, Critic, Experimenter), ARES automatically generates, evaluates, and optimizes code solutions for specified problems.

This document outlines the system configuration parameters, problem definition methods, and LLM interface settings.

## 1. Core System Configuration (`cfg/config.yaml`)

The system's core behavior is controlled via the `cfg/config.yaml` file. Below are the details for each key parameter:

### Environment & Parallelism
| Parameter | Description |
| :--- | :--- |
| **`problem_name`** | The name of the current task (must match the folder name in `problems/`). |
| **`description`** | A brief description of the current task. |
| **`num_eval_workers`** | **CPU Core Count**. Specifies the number of CPU cores used for parallel code evaluation. Increasing this speeds up the evaluation process. |
| **`timeout`** | **Execution Timeout** (in seconds). The maximum allowed time for a single code run. Execution is considered a failure if it exceeds this time. |

### Evolutionary Parameters
| Parameter | Description |
| :--- | :--- |
| **`init_pop_size`** | **Initial Population Size**. The number of offspring generated in the very first generation of the evolutionary algorithm. |
| **`pop_size`** | **Population Size**. The number of offspring generated in each subsequent generation during the evolution process. |
| **`max_fe`** | **Max Function Evaluations**. The total number of offspring codes generated during the entire run. This serves as the stopping criterion for the system. |
| **`eval_runs`** | **Evaluation Runs**. For heuristic algorithms with unstable results, this sets how many times each code is run. The system uses the **minimum value** (for minimization problems) or the **average** to assess performance. |
| **`stagnation_threshold`** | **Stagnation Threshold**. The minimum improvement required to consider a new best value as valid (e.g., `1e-4`). Improvements below this are treated as stagnation. |

### Advanced Exploration & Reflection
| Parameter | Description |
| :--- | :--- |
| **`meta_reflection_cycle_period`** | **Meta-Reflection Period**. Sets the number of iterations between triggers of the Meta-Reflection mechanism to summarize historical strategies. |
| **`radical_exploration_trigger`** | **Radical Exploration Trigger**. The number of consecutive stagnant iterations required to trigger the Radical Exploration mode. |
| **`radical_exploration_duration`** | **Radical Exploration Duration**. The number of iterations the system remains in Radical Exploration mode to escape local optima. |

---

## 2. Problem Definition & Initialization

To run a new problem, you must configure the name in `config.yaml` and inject domain knowledge into the Prompt template.

### Key Step
Please edit the following file:
> **`prompts/ares/user_generator_theorist_init.txt`**

At the **very beginning** of this file, you must provide:
1.  **Detailed Problem Description**: Clearly define the target task.
2.  **Function Interface Definition**: List available APIs, data structures, or function signatures that must be implemented.

**The "Theorist" role will use the description provided here to initialize the strategy table and generate the first generation of code.**

---

## 3. LLM Interface Settings

ARES supports multiple LLM backends. API keys and model selections are configured in the following locations:

1.  **Model Specific Config**:
    *   Navigate to the `cfg/llm_client` directory.
    *   Modify the configuration files corresponding to the models you are using (e.g., GPT-4, Claude).

2.  **Global Model Selection**:
    *   In `cfg/config.yaml`, reference the configurations above to specify which model client is used for each role (Theorist, Critic, Experimenter).

---

## 4. Quick Start

1.  Configure your API Keys in `cfg/llm_client`.
2.  Fill in the problem description and interface details in `prompts/ares/user_generator_theorist_init.txt`.
3.  Modify `cfg/config.yaml` to set the `problem_name` and evolutionary parameters (e.g., `pop_size`, `max_fe`).
4.  Run the main program to start ARES.

---

## 5. Adding Custom Problems

ARES is designed to minimize the complexity of adding new problem benchmarks. Follow these steps to integrate a new problem into the system:

### Step 1: Benchmark Setup (`problems/`)
Create a new folder in the `problems/` directory with your problem name (e.g., `problems/my_new_task`). This folder **must** contain the following two files:

*   **`gpt.py`**:
    *   A placeholder file used by ARES.
    *   In every generation, ARES writes the code generated by the LLM into this file for execution.
*   **`eval.py`**:
    *   The evaluation script for the problem.
    *   **Crucial Requirement**: The script must print the **Fitness Value** (a single float number) as the **very last line** of its standard output (stdout). ARES reads this last line to score the individual.

### Step 2: Prompt Context (`prompts/`)
Create a corresponding folder in the `prompts/` directory. **For simplicity, use the exact same name as your problems folder** (e.g., `prompts/my_new_task`).

You must create three text files here, referencing existing examples (e.g., `func_signature.txt`, `func_desc.txt`, `seed_func.txt`):
1.  **Function Signature**: Defines the required input/output format.
2.  **Function Description**: Provides context and rules for the problem.
3.  **Seed Function**: A basic valid solution or starting point.

### Step 3: Configuration (`cfg/problem/`)
The simplest way to configure a new problem is to maintain **consistent naming** across all directories.

1.  **Uniform Naming**: Ensure your benchmark folder in `problems/` and your prompt folder in `prompts/` share the **exact same name** (e.g., `my_new_task`).
2.  **Duplicate & Rename**: Go to `cfg/problem/`, copy an existing configuration file (as a template), and rename it to match your problem name (e.g., `my_new_task.yaml`).
3.  **Update Content**: Open your new `.yaml` file and simply update the `problem_name` and `problem_type` field to match the filename (e.g., `problem_name: my_new_task`).

### Step 4: Integration
Once the above steps are complete, simply update `problem_name` in the main `cfg/config.yaml` to your new problem's name to start the evolution.

